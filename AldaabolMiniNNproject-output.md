# ğŸ“ PROJECT EXPORT FOR LLMs

## ğŸ“Š Project Information

- **Project Name**: `AldaabolMiniNNproject`
- **Generated On**: 2025-12-27 23:40:17 (Asia/Riyadh / GMT+03:00)
- **Total Files Processed**: 45
- **Export Tool**: Easy Whole Project to Single Text File for LLMs v1.1.0
- **Tool Author**: Jota / JosÃ© Guilherme Pandolfi

### âš™ï¸ Export Configuration

| Setting | Value |
|---------|-------|
| Language | `en` |
| Max File Size | `1 MB` |
| Include Hidden Files | `false` |
| Output Format | `both` |

## ğŸŒ³ Project Structure

```
â”œâ”€â”€ ğŸ“ AldaabolMiniNN/
â”‚   â”œâ”€â”€ ğŸ“ __pycache__/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.cpython-314.pyc (1.22 KB)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ network.cpython-314.pyc (1.85 KB)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ trainer.cpython-314.pyc (3.36 KB)
â”‚   â”‚   â””â”€â”€ ğŸ“„ tuner.cpython-314.pyc (3.19 KB)
â”‚   â”œâ”€â”€ ğŸ“ layers/
â”‚   â”‚   â”œâ”€â”€ ğŸ“ __pycache__/
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.cpython-314.pyc (170 B)
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ activations.cpython-314.pyc (2.85 KB)
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ batch_norm.cpython-314.pyc (3.87 KB)
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ dense.cpython-314.pyc (2.26 KB)
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ dropout.cpython-314.pyc (1.86 KB)
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ layer.cpython-314.pyc (1.24 KB)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ activations.py (1.18 KB)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ batch_norm.py (2.05 KB)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ dense.py (1.06 KB)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ dropout.py (1022 B)
â”‚   â”‚   â””â”€â”€ ğŸ“„ layer.py (594 B)
â”‚   â”œâ”€â”€ ğŸ“ losses/
â”‚   â”‚   â”œâ”€â”€ ğŸ“ __pycache__/
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.cpython-314.pyc (192 B)
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ mse.cpython-314.pyc (1.12 KB)
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ softmax_ce.cpython-314.pyc (1.62 KB)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ mse.py (426 B)
â”‚   â”‚   â””â”€â”€ ğŸ“„ softmax_ce.py (634 B)
â”‚   â”œâ”€â”€ ğŸ“ optimizers/
â”‚   â”‚   â”œâ”€â”€ ğŸ“ __pycache__/
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.cpython-314.pyc (174 B)
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ adagrad.cpython-314.pyc (2.27 KB)
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ adam.cpython-314.pyc (3.93 KB)
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ momentum.cpython-314.pyc (2.05 KB)
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ optimizer.cpython-314.pyc (605 B)
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ sgd.cpython-314.pyc (1.16 KB)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ adagrad.py (1.12 KB)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ adam.py (1.87 KB)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ momentum.py (1.11 KB)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ optimizer.py (83 B)
â”‚   â”‚   â””â”€â”€ ğŸ“„ sgd.py (355 B)
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py (737 B)
â”‚   â”œâ”€â”€ ğŸ“„ network.py (902 B)
â”‚   â”œâ”€â”€ ğŸ“„ trainer.py (2.1 KB)
â”‚   â””â”€â”€ ğŸ“„ tuner.py (2.5 KB)
â”œâ”€â”€ ğŸ“ test/
â”‚   â”œâ”€â”€ ğŸ“ __pycache__/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.cpython-314.pyc (175 B)
â”‚   â”‚   â””â”€â”€ ğŸ“„ tester.cpython-314.pyc (2.25 KB)
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â””â”€â”€ ğŸ“„ tester.py (1.66 KB)
â”œâ”€â”€ ğŸ“„ LICENSE.txt (1.07 KB)
â”œâ”€â”€ ğŸ“„ README.md (6.49 KB)
â””â”€â”€ ğŸ“„ setup.py (755 B)
```

## ğŸ“‘ Table of Contents

**Project Files:**

- [ğŸ“„ AldaabolMiniNN/layers/__init__.py](#ğŸ“„-aldaabolmininn-layers-init-py)
- [ğŸ“„ AldaabolMiniNN/layers/activations.py](#ğŸ“„-aldaabolmininn-layers-activations-py)
- [ğŸ“„ AldaabolMiniNN/layers/batch_norm.py](#ğŸ“„-aldaabolmininn-layers-batch-norm-py)
- [ğŸ“„ AldaabolMiniNN/layers/dense.py](#ğŸ“„-aldaabolmininn-layers-dense-py)
- [ğŸ“„ AldaabolMiniNN/layers/dropout.py](#ğŸ“„-aldaabolmininn-layers-dropout-py)
- [ğŸ“„ AldaabolMiniNN/layers/layer.py](#ğŸ“„-aldaabolmininn-layers-layer-py)
- [ğŸ“„ AldaabolMiniNN/losses/__init__.py](#ğŸ“„-aldaabolmininn-losses-init-py)
- [ğŸ“„ AldaabolMiniNN/losses/mse.py](#ğŸ“„-aldaabolmininn-losses-mse-py)
- [ğŸ“„ AldaabolMiniNN/losses/softmax_ce.py](#ğŸ“„-aldaabolmininn-losses-softmax-ce-py)
- [ğŸ“„ AldaabolMiniNN/optimizers/__init__.py](#ğŸ“„-aldaabolmininn-optimizers-init-py)
- [ğŸ“„ AldaabolMiniNN/optimizers/adagrad.py](#ğŸ“„-aldaabolmininn-optimizers-adagrad-py)
- [ğŸ“„ AldaabolMiniNN/optimizers/adam.py](#ğŸ“„-aldaabolmininn-optimizers-adam-py)
- [ğŸ“„ AldaabolMiniNN/optimizers/momentum.py](#ğŸ“„-aldaabolmininn-optimizers-momentum-py)
- [ğŸ“„ AldaabolMiniNN/optimizers/optimizer.py](#ğŸ“„-aldaabolmininn-optimizers-optimizer-py)
- [ğŸ“„ AldaabolMiniNN/optimizers/sgd.py](#ğŸ“„-aldaabolmininn-optimizers-sgd-py)
- [ğŸ“„ AldaabolMiniNN/__init__.py](#ğŸ“„-aldaabolmininn-init-py)
- [ğŸ“„ AldaabolMiniNN/network.py](#ğŸ“„-aldaabolmininn-network-py)
- [ğŸ“„ AldaabolMiniNN/trainer.py](#ğŸ“„-aldaabolmininn-trainer-py)
- [ğŸ“„ AldaabolMiniNN/tuner.py](#ğŸ“„-aldaabolmininn-tuner-py)
- [ğŸ“„ test/__init__.py](#ğŸ“„-test-init-py)
- [ğŸ“„ test/tester.py](#ğŸ“„-test-tester-py)
- [ğŸ“„ LICENSE.txt](#ğŸ“„-license-txt)
- [ğŸ“„ README.md](#ğŸ“„-readme-md)
- [ğŸ“„ setup.py](#ğŸ“„-setup-py)

---

## ğŸ“ˆ Project Statistics

| Metric | Count |
|--------|-------|
| Total Files | 45 |
| Total Directories | 10 |
| Text Files | 24 |
| Binary Files | 21 |
| Total Size | 64.97 KB |

### ğŸ“„ File Types Distribution

| Extension | Count |
|-----------|-------|
| `.py` | 22 |
| `.pyc` | 21 |
| `.txt` | 1 |
| `.md` | 1 |

## ğŸ’» File Code Contents

## ğŸš« Binary/Excluded Files

The following files were not included in the text content:

- `AldaabolMiniNN/__pycache__/__init__.cpython-314.pyc`
- `AldaabolMiniNN/__pycache__/network.cpython-314.pyc`
- `AldaabolMiniNN/__pycache__/trainer.cpython-314.pyc`
- `AldaabolMiniNN/__pycache__/tuner.cpython-314.pyc`

## ğŸš« Binary/Excluded Files

The following files were not included in the text content:

- `AldaabolMiniNN/layers/__pycache__/__init__.cpython-314.pyc`
- `AldaabolMiniNN/layers/__pycache__/activations.cpython-314.pyc`
- `AldaabolMiniNN/layers/__pycache__/batch_norm.cpython-314.pyc`
- `AldaabolMiniNN/layers/__pycache__/dense.cpython-314.pyc`
- `AldaabolMiniNN/layers/__pycache__/dropout.cpython-314.pyc`
- `AldaabolMiniNN/layers/__pycache__/layer.cpython-314.pyc`

### <a id="ğŸ“„-aldaabolmininn-layers-init-py"></a>ğŸ“„ `AldaabolMiniNN/layers/__init__.py`

**File Info:**
- **Size**: 0 B
- **Extension**: `.py`
- **Language**: `python`
- **Location**: `AldaabolMiniNN/layers/__init__.py`
- **Relative Path**: `AldaabolMiniNN/layers`
- **Created**: 2025-12-26 23:17:08 (Asia/Riyadh / GMT+03:00)
- **Modified**: 2025-12-26 23:26:32 (Asia/Riyadh / GMT+03:00)
- **MD5**: `d41d8cd98f00b204e9800998ecf8427e`
- **SHA256**: `e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855`
- **Encoding**: ASCII

**File code content:**

```python

```

---

### <a id="ğŸ“„-aldaabolmininn-layers-activations-py"></a>ğŸ“„ `AldaabolMiniNN/layers/activations.py`

**File Info:**
- **Size**: 1.18 KB
- **Extension**: `.py`
- **Language**: `python`
- **Location**: `AldaabolMiniNN/layers/activations.py`
- **Relative Path**: `AldaabolMiniNN/layers`
- **Created**: 2025-12-26 23:18:25 (Asia/Riyadh / GMT+03:00)
- **Modified**: 2025-12-27 15:52:07 (Asia/Riyadh / GMT+03:00)
- **MD5**: `e3b21273f94821caa5273a83985c8de1`
- **SHA256**: `a75191072467b1a2ba81f029974f7e144be9115c06d28b3fb9e82ece5b6561fb`
- **Encoding**: ASCII

**File code content:**

```python
import numpy as np
from .layer import Layer

# Activation Functions
class ReLU(Layer):
    def forward(self, input_data):
        self.input = input_data
        return np.maximum(0, input_data)

    def backward(self, output_gradient):
        return output_gradient * (self.input > 0)

class Sigmoid(Layer):
    def forward(self, input_data):
        self.output = 1 / (1 + np.exp(-input_data))
        return self.output

    def backward(self, output_gradient):
        return output_gradient * (self.output * (1 - self.output))
    
class Tanh(Layer):
    def forward(self, input_data):
        self.output = np.tanh(input_data)
        return self.output

    def backward(self, output_gradient):
        return output_gradient * (1 - self.output ** 2)
    
class Softmax(Layer):
    def forward(self, input_data):
        exp_values = np.exp(input_data - np.max(input_data, axis=1, keepdims=True))
        self.output = exp_values / np.sum(exp_values, axis=1, keepdims=True)
        return self.output

    def backward(self, output_gradient):
        # Note: Softmax backward is usually combined with Cross-Entropy loss for efficiency
        return output_gradient  
```

---

### <a id="ğŸ“„-aldaabolmininn-layers-batch-norm-py"></a>ğŸ“„ `AldaabolMiniNN/layers/batch_norm.py`

**File Info:**
- **Size**: 2.05 KB
- **Extension**: `.py`
- **Language**: `python`
- **Location**: `AldaabolMiniNN/layers/batch_norm.py`
- **Relative Path**: `AldaabolMiniNN/layers`
- **Created**: 2025-12-26 23:19:42 (Asia/Riyadh / GMT+03:00)
- **Modified**: 2025-12-27 23:05:59 (Asia/Riyadh / GMT+03:00)
- **MD5**: `d134437facfe194911d8c455335610a2`
- **SHA256**: `7cb7e9358be468476a39aba597cfd1ef556ebf8e5d5331b503df18680c90fd4f`
- **Encoding**: UTF-8

**File code content:**

```python
import numpy as np
from .layer import Layer

class BatchNormalization(Layer):
    '''
    ÙƒÙ„Ø§Ø³ Ø¨ÙŠØ·Ø¨Ù‚ ØªÙ‚Ù†ÙŠØ© Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ Ù„Ù„Ø¯ÙØ¹Ø§Øª (Batch Normalization)
    Ø¹Ù„Ù‰ Ø§Ù„Ø·Ø¨Ù‚Ø© ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©. Ù‡Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø¨ØªØ³Ø§Ø¹Ø¯ ÙÙŠ ØªØ³Ø±ÙŠØ¹
    ÙˆØªØ­Ø³ÙŠÙ† Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ·Ø¨ÙŠØ¹ Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
    ÙˆØªÙ‚Ù„ÙŠÙ„ Ø§Ù„ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© ÙÙŠ Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠ Ù„Ù„Ù…Ø¯Ø®Ù„Ø§Øª.
    '''
    def __init__(self, epsilon=1e-8):
        super().__init__("BatchNormalization")
        self.epsilon = epsilon
        self.gamma = None
        self.beta = None
        self.input_normalized = None
        self.mean = None
        self.variance = None

    def forward(self, input_data):
        self.input = input_data 
        
        if self.gamma is None:
            self.gamma = np.ones((1, input_data.shape[1]))
        if self.beta is None:
            self.beta = np.zeros((1, input_data.shape[1]))

        self.mean = np.mean(input_data, axis=0)
        self.variance = np.var(input_data, axis=0)
        self.input_normalized = (input_data - self.mean) / np.sqrt(self.variance + self.epsilon)
        return self.gamma * self.input_normalized + self.beta

    def backward(self, output_gradient):
        m = output_gradient.shape[0]
        dbeta = np.sum(output_gradient, axis=0)
        dgamma = np.sum(output_gradient * self.input_normalized, axis=0)

        dinput_normalized = output_gradient * self.gamma
        dvariance = np.sum(dinput_normalized * (self.input - self.mean) * -0.5 * (self.variance + self.epsilon) ** -1.5, axis=0)
        dmean = np.sum(dinput_normalized * -1 / np.sqrt(self.variance + self.epsilon), axis=0) + dvariance * np.mean(-2 * (self.input - self.mean), axis=0)

        dinput = dinput_normalized / np.sqrt(self.variance + self.epsilon) + dvariance * 2 * (self.input - self.mean) / m + dmean / m

        self.gamma_grad = dgamma
        self.beta_grad = dbeta
        return dinput
```

---

### <a id="ğŸ“„-aldaabolmininn-layers-dense-py"></a>ğŸ“„ `AldaabolMiniNN/layers/dense.py`

**File Info:**
- **Size**: 1.06 KB
- **Extension**: `.py`
- **Language**: `python`
- **Location**: `AldaabolMiniNN/layers/dense.py`
- **Relative Path**: `AldaabolMiniNN/layers`
- **Created**: 2025-12-26 23:17:36 (Asia/Riyadh / GMT+03:00)
- **Modified**: 2025-12-27 16:05:03 (Asia/Riyadh / GMT+03:00)
- **MD5**: `9db656978f565e93a3a056d83fbcb501`
- **SHA256**: `0b9f530599f21c3c948bf3f4ac0bdedc318c02c26e4db9aa7fe3dfb58979cee5`
- **Encoding**: UTF-8

**File code content:**

```python
import numpy as np
from .layer import Layer

class Dense(Layer):
    '''
    Ù‡Ø§Ø¯ Ø§Ù„ÙƒÙ„Ø§Ø³ Ø¨ÙŠÙ…Ø«Ù„ Ø·Ø¨Ù‚Ø© Dense (Fully Connected) ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©.
    Ø¨ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£ÙˆØ²Ø§Ù† ÙˆØ§Ù†Ø­ÙŠØ§Ø²Ø§ØªØŒ ÙˆØ¨ÙŠÙˆÙØ± ØªÙˆØ§Ø¨Ø¹ Ù„Ù„Ù€ (forward pass)
    ÙˆØ§Ù„Ù€ (backward pass)
    Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø®Ù„Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨.
    '''
    def __init__(self, input_size, output_size):
        super().__init__("Dense")
        self.weights = np.random.randn(input_size, output_size) * np.sqrt(2./input_size)
        self.biases = np.zeros((1, output_size))
        self.weights_grad = None
        self.biases_grad = None

    def forward(self, input_data):
        self.input = input_data
        return np.dot(input_data, self.weights) + self.biases

    def backward(self, output_gradient):
        self.weights_grad = np.dot(self.input.T, output_gradient)
        self.biases_grad = np.sum(output_gradient, axis=0, keepdims=True)
        return np.dot(output_gradient, self.weights.T)
```

---

### <a id="ğŸ“„-aldaabolmininn-layers-dropout-py"></a>ğŸ“„ `AldaabolMiniNN/layers/dropout.py`

**File Info:**
- **Size**: 1022 B
- **Extension**: `.py`
- **Language**: `python`
- **Location**: `AldaabolMiniNN/layers/dropout.py`
- **Relative Path**: `AldaabolMiniNN/layers`
- **Created**: 2025-12-26 23:19:14 (Asia/Riyadh / GMT+03:00)
- **Modified**: 2025-12-27 16:02:46 (Asia/Riyadh / GMT+03:00)
- **MD5**: `eab954d33e46d8d1d187ee03afebf266`
- **SHA256**: `f7d31f2a5c47df40dd1fbdd8927282a5718d1f70c3fe4f7d81d94ebea529cfe3`
- **Encoding**: UTF-8

**File code content:**

```python
import numpy as np
from .layer import Layer

class Dropout(Layer):
    '''
    ÙƒÙ„Ø§Ø³ Ø¨ÙŠØ·Ø¨Ù‚ ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø¥Ø³Ù‚Ø§Ø· (Dropout) Ø¹Ù„Ù‰ Ø§Ù„Ø·Ø¨Ù‚Ø© ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©
    Ø®Ù„Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ Ø¨ÙŠØªÙ… Ø¥Ø³Ù‚Ø§Ø· Ø¨Ø¹Ø¶ Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø¨Ø´ÙƒÙ„ Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰
    Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ø¥Ø³Ù‚Ø§Ø· Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©ØŒ Ù…Ù…Ø§ ÙŠØ³Ø§Ø¹Ø¯ ÙÙŠ ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ù€ (overfitting)
    ÙˆÙ‡ÙŠÙƒ Ø¨Ø¹Ø²Ø² Ù…Ù† Ù‚Ø¯Ø±Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù…ÙŠÙ…
    '''
    def __init__(self, drop_probability):
        super().__init__("Dropout")
        self.drop_probability = drop_probability
        self.mask = None

    def forward(self, input_data, training=True):
        if training:
            self.mask = (np.random.rand(*input_data.shape) > self.drop_probability) / (1.0 - self.drop_probability)
            return input_data * self.mask
        else:
            return input_data

    def backward(self, output_gradient):
        return output_gradient * self.mask
```

---

### <a id="ğŸ“„-aldaabolmininn-layers-layer-py"></a>ğŸ“„ `AldaabolMiniNN/layers/layer.py`

**File Info:**
- **Size**: 594 B
- **Extension**: `.py`
- **Language**: `python`
- **Location**: `AldaabolMiniNN/layers/layer.py`
- **Relative Path**: `AldaabolMiniNN/layers`
- **Created**: 2025-12-26 23:17:16 (Asia/Riyadh / GMT+03:00)
- **Modified**: 2025-12-27 23:06:20 (Asia/Riyadh / GMT+03:00)
- **MD5**: `e2511236e84b727b59ac0dfe5a758648`
- **SHA256**: `caafe2bc4a93782a39c8d502e8a2ee6af363dbf29b8f843b927ccbef50123582`
- **Encoding**: UTF-8

**File code content:**

```python
class Layer:
    '''
    Ø§Ù„ÙƒÙ„Ø§Ø³ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„ÙƒÙ„ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©
    ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØ§Ø¨Ø¹ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø£Ù† ØªÙ†ÙØ°Ù‡Ø§ ÙƒÙ„ Ø·Ø¨Ù‚Ø©
    ÙƒÙ„ Ø·Ø¨Ù‚Ø© Ø¨ØªÙ… ØªØ¹Ø±ÙŠÙÙ‡Ø§ Ø¨ØªØ±Ø« Ù…Ù† Ù‡Ø§Ø¯ Ø§Ù„ÙƒÙ„Ø§Ø³
    Ø¨Ø­ÙŠØ« Ø¨ÙŠÙØ±Ø¶ Ø¹Ù„ÙŠÙ‡Ø§ ØªÙ†ÙÙŠØ° Ø§Ù„ØªÙˆØ§Ø¨Ø¹ forward Ùˆ backward
    '''
    def __init__(self, name=None):
        self.name = name

    def forward(self, input_data):
        raise NotImplementedError

    def backward(self, output_gradient):
        raise NotImplementedError

```

---

## ğŸš« Binary/Excluded Files

The following files were not included in the text content:

- `AldaabolMiniNN/losses/__pycache__/__init__.cpython-314.pyc`
- `AldaabolMiniNN/losses/__pycache__/mse.cpython-314.pyc`
- `AldaabolMiniNN/losses/__pycache__/softmax_ce.cpython-314.pyc`

### <a id="ğŸ“„-aldaabolmininn-losses-init-py"></a>ğŸ“„ `AldaabolMiniNN/losses/__init__.py`

**File Info:**
- **Size**: 0 B
- **Extension**: `.py`
- **Language**: `python`
- **Location**: `AldaabolMiniNN/losses/__init__.py`
- **Relative Path**: `AldaabolMiniNN/losses`
- **Created**: 2025-12-26 23:26:08 (Asia/Riyadh / GMT+03:00)
- **Modified**: 2025-12-27 09:27:09 (Asia/Riyadh / GMT+03:00)
- **MD5**: `d41d8cd98f00b204e9800998ecf8427e`
- **SHA256**: `e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855`
- **Encoding**: ASCII

**File code content:**

```python

```

---

### <a id="ğŸ“„-aldaabolmininn-losses-mse-py"></a>ğŸ“„ `AldaabolMiniNN/losses/mse.py`

**File Info:**
- **Size**: 426 B
- **Extension**: `.py`
- **Language**: `python`
- **Location**: `AldaabolMiniNN/losses/mse.py`
- **Relative Path**: `AldaabolMiniNN/losses`
- **Created**: 2025-12-26 23:20:34 (Asia/Riyadh / GMT+03:00)
- **Modified**: 2025-12-27 09:19:17 (Asia/Riyadh / GMT+03:00)
- **MD5**: `9de2026f2b8ad18eeb30e5b969638c5a`
- **SHA256**: `e88021e26aac452d4679c0025459cff1cb82f4ac983855e0eaa46a4b598c9d6d`
- **Encoding**: UTF-8

**File code content:**

```python
import numpy as np

class MSE:
    '''     
    Ù‡Ø§Ø¯ Ø§Ù„ÙƒÙ„Ø§Ø³ Ø¨ÙŠØ­Ø³Ø¨ Ø®Ø³Ø§Ø±Ø© Ù…ØªÙˆØ³Ø· Ù…Ø±Ø¨Ø¹ Ø§Ù„Ø®Ø·Ø£ (Mean Squared Error) Ø¨ÙŠÙ† Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© ÙˆØ§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©.
    '''
    @staticmethod
    def loss(y_true, y_pred):
        return np.mean((y_true - y_pred) ** 2)

    @staticmethod
    def backward(y_true, y_pred):
        return 2 * (y_pred - y_true) / y_true.size
```

---

### <a id="ğŸ“„-aldaabolmininn-losses-softmax-ce-py"></a>ğŸ“„ `AldaabolMiniNN/losses/softmax_ce.py`

**File Info:**
- **Size**: 634 B
- **Extension**: `.py`
- **Language**: `python`
- **Location**: `AldaabolMiniNN/losses/softmax_ce.py`
- **Relative Path**: `AldaabolMiniNN/losses`
- **Created**: 2025-12-26 23:21:14 (Asia/Riyadh / GMT+03:00)
- **Modified**: 2025-12-27 17:36:32 (Asia/Riyadh / GMT+03:00)
- **MD5**: `6d88228525856e2a718f681e9151e6f2`
- **SHA256**: `a9b0f72137f5f14373dd89bdd2267e042be62d8fe4bb0423b1e658a090143286`
- **Encoding**: UTF-8

**File code content:**

```python
import numpy as np

class SoftmaxCrossEntropy:
    '''
    Ù‡Ø§Ø¯ Ø§Ù„ÙƒÙ„Ø§Ø³ Ø¨ÙŠØ­Ø³Ø¨ Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ù€ Softmax Ù…Ø¹ Cross-Entropy Ø¨ÙŠÙ† Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© ÙˆØ§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©.
    '''
    def loss(self, y_true, y_pred):
        exps = np.exp(y_pred - np.max(y_pred, axis=1, keepdims=True))
        self.softmax = exps / np.sum(exps, axis=1, keepdims=True)
        
        m = y_true.shape[0]
        loss = -np.sum(y_true * np.log(self.softmax + 1e-9)) / m
        return loss

    def backward(self, y_true, y_pred):
        m = y_true.shape[0]
        return (self.softmax - y_true) / m


```

---

## ğŸš« Binary/Excluded Files

The following files were not included in the text content:

- `AldaabolMiniNN/optimizers/__pycache__/__init__.cpython-314.pyc`
- `AldaabolMiniNN/optimizers/__pycache__/adagrad.cpython-314.pyc`
- `AldaabolMiniNN/optimizers/__pycache__/adam.cpython-314.pyc`
- `AldaabolMiniNN/optimizers/__pycache__/momentum.cpython-314.pyc`
- `AldaabolMiniNN/optimizers/__pycache__/optimizer.cpython-314.pyc`
- `AldaabolMiniNN/optimizers/__pycache__/sgd.cpython-314.pyc`

### <a id="ğŸ“„-aldaabolmininn-optimizers-init-py"></a>ğŸ“„ `AldaabolMiniNN/optimizers/__init__.py`

**File Info:**
- **Size**: 0 B
- **Extension**: `.py`
- **Language**: `python`
- **Location**: `AldaabolMiniNN/optimizers/__init__.py`
- **Relative Path**: `AldaabolMiniNN/optimizers`
- **Created**: 2025-12-26 23:26:16 (Asia/Riyadh / GMT+03:00)
- **Modified**: 2025-12-26 23:26:25 (Asia/Riyadh / GMT+03:00)
- **MD5**: `d41d8cd98f00b204e9800998ecf8427e`
- **SHA256**: `e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855`
- **Encoding**: ASCII

**File code content:**

```python

```

---

### <a id="ğŸ“„-aldaabolmininn-optimizers-adagrad-py"></a>ğŸ“„ `AldaabolMiniNN/optimizers/adagrad.py`

**File Info:**
- **Size**: 1.12 KB
- **Extension**: `.py`
- **Language**: `python`
- **Location**: `AldaabolMiniNN/optimizers/adagrad.py`
- **Relative Path**: `AldaabolMiniNN/optimizers`
- **Created**: 2025-12-26 23:23:29 (Asia/Riyadh / GMT+03:00)
- **Modified**: 2025-12-27 08:14:41 (Asia/Riyadh / GMT+03:00)
- **MD5**: `ae0b97242ea7f9185165ff406a4ba81e`
- **SHA256**: `cd57f65c82d30ec4deec4c58af4b7110529183a61b165ef316acad4147f92db0`
- **Encoding**: ASCII

**File code content:**

```python

from .optimizer import Optimizer
import numpy as np

class AdaGrad(Optimizer):
    def __init__(self, learning_rate=0.01, epsilon=1e-8):
        self.learning_rate = learning_rate
        self.epsilon = epsilon
        self.sum_squared_weights = {}
        self.sum_squared_biases = {}

    def update(self, layer):
        if hasattr(layer, 'weights'):
            if layer not in self.sum_squared_weights:
                self.sum_squared_weights[layer] = np.zeros_like(layer.weights)
                self.sum_squared_biases[layer] = np.zeros_like(layer.biases)

            # Update weights
            self.sum_squared_weights[layer] += layer.weights_grad ** 2
            adjusted_lr_weights = self.learning_rate / (np.sqrt(self.sum_squared_weights[layer]) + self.epsilon)
            layer.weights -= adjusted_lr_weights * layer.weights_grad

            # Update biases
            self.sum_squared_biases[layer] += layer.biases_grad ** 2
            adjusted_lr_biases = self.learning_rate / (np.sqrt(self.sum_squared_biases[layer]) + self.epsilon)
            layer.biases -= adjusted_lr_biases * layer.biases_grad
```

---

### <a id="ğŸ“„-aldaabolmininn-optimizers-adam-py"></a>ğŸ“„ `AldaabolMiniNN/optimizers/adam.py`

**File Info:**
- **Size**: 1.87 KB
- **Extension**: `.py`
- **Language**: `python`
- **Location**: `AldaabolMiniNN/optimizers/adam.py`
- **Relative Path**: `AldaabolMiniNN/optimizers`
- **Created**: 2025-12-26 23:25:21 (Asia/Riyadh / GMT+03:00)
- **Modified**: 2025-12-27 08:14:25 (Asia/Riyadh / GMT+03:00)
- **MD5**: `e6abdb660e61f7944ebe62fc5728046a`
- **SHA256**: `c7021fc1554dbb66de5458f52fc3129504402ac95787b5ea9df73d59f8ee7706`
- **Encoding**: ASCII

**File code content:**

```python
    
import numpy as np
from .optimizer import Optimizer


class Adam(Optimizer):
    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
        self.learning_rate = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.m_weights = {}
        self.v_weights = {}
        self.m_biases = {}
        self.v_biases = {}
        self.t = 0

    def update(self, layer):
        if hasattr(layer, 'weights'):
            if layer not in self.m_weights:
                self.m_weights[layer] = np.zeros_like(layer.weights)
                self.v_weights[layer] = np.zeros_like(layer.weights)
                self.m_biases[layer] = np.zeros_like(layer.biases)
                self.v_biases[layer] = np.zeros_like(layer.biases)

            self.t += 1

            # Update weights
            self.m_weights[layer] = self.beta1 * self.m_weights[layer] + (1 - self.beta1) * layer.weights_grad
            self.v_weights[layer] = self.beta2 * self.v_weights[layer] + (1 - self.beta2) * (layer.weights_grad ** 2)

            m_hat_weights = self.m_weights[layer] / (1 - self.beta1 ** self.t)
            v_hat_weights = self.v_weights[layer] / (1 - self.beta2 ** self.t)

            layer.weights -= self.learning_rate * m_hat_weights / (np.sqrt(v_hat_weights) + self.epsilon)

            # Update biases
            self.m_biases[layer] = self.beta1 * self.m_biases[layer] + (1 - self.beta1) * layer.biases_grad
            self.v_biases[layer] = self.beta2 * self.v_biases[layer] + (1 - self.beta2) * (layer.biases_grad ** 2)

            m_hat_biases = self.m_biases[layer] / (1 - self.beta1 ** self.t)
            v_hat_biases = self.v_biases[layer] / (1 - self.beta2 ** self.t)

            layer.biases -= self.learning_rate * m_hat_biases / (np.sqrt(v_hat_biases) + self.epsilon)


```

---

### <a id="ğŸ“„-aldaabolmininn-optimizers-momentum-py"></a>ğŸ“„ `AldaabolMiniNN/optimizers/momentum.py`

**File Info:**
- **Size**: 1.11 KB
- **Extension**: `.py`
- **Language**: `python`
- **Location**: `AldaabolMiniNN/optimizers/momentum.py`
- **Relative Path**: `AldaabolMiniNN/optimizers`
- **Created**: 2025-12-26 23:24:18 (Asia/Riyadh / GMT+03:00)
- **Modified**: 2025-12-27 08:14:45 (Asia/Riyadh / GMT+03:00)
- **MD5**: `4decd4962ccbb75cf8e42a9700605fec`
- **SHA256**: `444638876bce4a488b99c5bb5f3c75021490a54851022c4711c3a2e199b5c5cf`
- **Encoding**: ASCII

**File code content:**

```python

    
from .optimizer import Optimizer

import numpy as np  

class Momentum(Optimizer):
    def __init__(self, learning_rate=0.01, momentum=0.9):
        self.learning_rate = learning_rate
        self.momentum = momentum
        self.velocities_weights = {}
        self.velocities_biases = {}

    def update(self, layer):
        if hasattr(layer, 'weights'):
            if layer not in self.velocities_weights:
                self.velocities_weights[layer] = np.zeros_like(layer.weights)
                self.velocities_biases[layer] = np.zeros_like(layer.biases)

            # Update weights
            self.velocities_weights[layer] = (self.momentum * self.velocities_weights[layer] - 
                                              self.learning_rate * layer.weights_grad)
            layer.weights += self.velocities_weights[layer]

            # Update biases
            self.velocities_biases[layer] = (self.momentum * self.velocities_biases[layer] - 
                                             self.learning_rate * layer.biases_grad)
            layer.biases += self.velocities_biases[layer]

```

---

### <a id="ğŸ“„-aldaabolmininn-optimizers-optimizer-py"></a>ğŸ“„ `AldaabolMiniNN/optimizers/optimizer.py`

**File Info:**
- **Size**: 83 B
- **Extension**: `.py`
- **Language**: `python`
- **Location**: `AldaabolMiniNN/optimizers/optimizer.py`
- **Relative Path**: `AldaabolMiniNN/optimizers`
- **Created**: 2025-12-26 23:22:50 (Asia/Riyadh / GMT+03:00)
- **Modified**: 2025-12-26 23:22:53 (Asia/Riyadh / GMT+03:00)
- **MD5**: `4103c999549d8da97b69994cdcafd928`
- **SHA256**: `3dbeee0fa2ed4f48af0fb2a2ec2d7d0f7bb02019b345e0d05881bf70d6166535`
- **Encoding**: ASCII

**File code content:**

```python
class Optimizer:
    def update(self, layer):
        raise NotImplementedError  
```

---

### <a id="ğŸ“„-aldaabolmininn-optimizers-sgd-py"></a>ğŸ“„ `AldaabolMiniNN/optimizers/sgd.py`

**File Info:**
- **Size**: 355 B
- **Extension**: `.py`
- **Language**: `python`
- **Location**: `AldaabolMiniNN/optimizers/sgd.py`
- **Relative Path**: `AldaabolMiniNN/optimizers`
- **Created**: 2025-12-26 23:22:24 (Asia/Riyadh / GMT+03:00)
- **Modified**: 2025-12-27 08:14:05 (Asia/Riyadh / GMT+03:00)
- **MD5**: `25e63805d76188ef084c5e60d558fa54`
- **SHA256**: `ce4aca2ad7473479edc1ad6d78306f430f600edce4125ebd9c19760770d3721d`
- **Encoding**: ASCII

**File code content:**

```python
from .optimizer import Optimizer

class SGD(Optimizer):
    def __init__(self, learning_rate=0.01):
        self.learning_rate = learning_rate

    def update(self, layer):
        if hasattr(layer, 'weights'):
            layer.weights -= self.learning_rate * layer.weights_grad
            layer.biases -= self.learning_rate * layer.biases_grad
```

---

### <a id="ğŸ“„-aldaabolmininn-init-py"></a>ğŸ“„ `AldaabolMiniNN/__init__.py`

**File Info:**
- **Size**: 737 B
- **Extension**: `.py`
- **Language**: `python`
- **Location**: `AldaabolMiniNN/__init__.py`
- **Relative Path**: `AldaabolMiniNN`
- **Created**: 2025-12-27 07:40:42 (Asia/Riyadh / GMT+03:00)
- **Modified**: 2025-12-27 15:52:05 (Asia/Riyadh / GMT+03:00)
- **MD5**: `229d95020f93f7423e6132904ac0af6f`
- **SHA256**: `cb8e51dc7c94d63caa47fdb9ef591a0cce99117f1c4edee017c438bff2f19bdd`
- **Encoding**: ASCII

**File code content:**

```python

import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from .layers.dense import Dense
from .layers.activations import Sigmoid
from .layers.activations import ReLU
from .layers.activations import Tanh
from .layers.batch_norm import BatchNormalization
from .layers.activations import Softmax
from .layers.dropout import Dropout

from .network import NeuralNetwork
from .trainer import Trainer
from .tuner import HyperparameterTuning


from .optimizers.sgd import SGD
from .optimizers.momentum import Momentum
from .optimizers.adam import Adam
from .optimizers.adagrad import AdaGrad

from .losses.mse import MSE
from .losses.softmax_ce import SoftmaxCrossEntropy


```

---

### <a id="ğŸ“„-aldaabolmininn-network-py"></a>ğŸ“„ `AldaabolMiniNN/network.py`

**File Info:**
- **Size**: 902 B
- **Extension**: `.py`
- **Language**: `python`
- **Location**: `AldaabolMiniNN/network.py`
- **Relative Path**: `AldaabolMiniNN`
- **Created**: 2025-12-26 23:26:47 (Asia/Riyadh / GMT+03:00)
- **Modified**: 2025-12-27 16:05:10 (Asia/Riyadh / GMT+03:00)
- **MD5**: `93c29d8d57ea7a7b9834fb1dc454ce4d`
- **SHA256**: `6629ce779d4d8def70cbd45997cf06719076a2157605ad47b39b1ab21fef2a03`
- **Encoding**: UTF-8

**File code content:**

```python
class NeuralNetwork:
    """
    ÙƒÙ„Ø§Ø³ Ø¨Ù…Ø«Ù„ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©
    Ø¨ÙŠØ­ØªÙˆÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø·Ø¨Ù‚Ø§Øª layers 
    ÙˆØ¨ÙŠÙˆÙØ± ØªØ§Ø¨Ø¹ Ù„Ù„ØªÙ†Ø¨Ø¤ ÙˆØªØ§Ø¨Ø¹ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    """
    def __init__(self, layers=None):
        self.layers = layers if layers is not None else []

    def add(self, layer):
        self.layers.append(layer)

    def predict(self, input_data):
        output = input_data
        for layer in self.layers:
            output = layer.forward(output)
        return output

    def train_step(self, x_batch, y_batch, loss_fn, optimizer):
        y_pred = self.predict(x_batch)
        loss = loss_fn.loss(y_batch, y_pred)
        
        grad = loss_fn.backward(y_batch, y_pred)
        for layer in reversed(self.layers):
            grad = layer.backward(grad)
            optimizer.update(layer)
        return loss
```

---

### <a id="ğŸ“„-aldaabolmininn-trainer-py"></a>ğŸ“„ `AldaabolMiniNN/trainer.py`

**File Info:**
- **Size**: 2.1 KB
- **Extension**: `.py`
- **Language**: `python`
- **Location**: `AldaabolMiniNN/trainer.py`
- **Relative Path**: `AldaabolMiniNN`
- **Created**: 2025-12-26 23:27:08 (Asia/Riyadh / GMT+03:00)
- **Modified**: 2025-12-27 17:05:24 (Asia/Riyadh / GMT+03:00)
- **MD5**: `2b71bd10346472a25a454deab48c125f`
- **SHA256**: `b956b0f70aedae163ee078beae7cc0b168ba6dc405f2fc19974b4b083ea8ac8d`
- **Encoding**: UTF-8

**File code content:**

```python
import numpy as np
class Trainer:
    '''
    Ù‡Ø§Ø¯ Ø§Ù„ÙƒÙ„Ø§Ø³ Ø¨ÙŠØ´Ø±Ù Ø¹Ù„Ù‰ Ø¹Ù…Ù„ÙŠØ© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©.
    Ø¨ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ù€ Optimizer Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙˆØ²Ø§Ù† ÙˆØ¨ÙŠØ­Ø³Ø¨ Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯Ø§Ù„Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.
    
    '''
    def __init__(self, model, optimizer, loss_fn):
        self.model = model
        self.optimizer = optimizer
        self.loss_fn = loss_fn

    def train_step(self, x_batch, y_batch):
        # forward pass
        y_pred = self.model.predict(x_batch)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø³Ø§Ø±Ø©
        loss = self.loss_fn.loss(y_batch, y_pred)
        
        # backward pass
        grad = self.loss_fn.backward(y_batch, y_pred)
        for layer in reversed(self.model.layers):
            grad = layer.backward(grad)
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù€ Optimizer 
            self.optimizer.update(layer)
            
        return loss

    def fit(self, x_train, y_train, x_val, y_val, epochs, batch_size=32):
        for epoch in range(epochs + 1):
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Batches 
            indices = np.random.permutation(len(x_train))
            x_shuffled = x_train[indices]
            y_shuffled = y_train[indices]
            
            for i in range(0, len(x_train), batch_size):
                x_batch = x_shuffled[i:i+batch_size]
                y_batch = y_shuffled[i:i+batch_size]
                loss = self.train_step(x_batch, y_batch)
            
            if epoch % 100 == 0:
                acc = self.calculate_accuracy(x_val, y_val)
                print(f"Epoch {epoch}:   Loss {loss:.4f},   Validation Accuracy: {acc:.2f}%")

    def calculate_accuracy(self, x, y):
        predictions = self.model.predict(x)
        y_pred_labels = np.argmax(predictions, axis=1)
        y_true_labels = np.argmax(y, axis=1)
        return np.mean(y_pred_labels == y_true_labels) * 100
    
    def calculate_loss(self, x, y):
        predictions = self.model.predict(x)
        return self.loss_fn.loss(predictions, y)
```

---

### <a id="ğŸ“„-aldaabolmininn-tuner-py"></a>ğŸ“„ `AldaabolMiniNN/tuner.py`

**File Info:**
- **Size**: 2.5 KB
- **Extension**: `.py`
- **Language**: `python`
- **Location**: `AldaabolMiniNN/tuner.py`
- **Relative Path**: `AldaabolMiniNN`
- **Created**: 2025-12-26 23:27:36 (Asia/Riyadh / GMT+03:00)
- **Modified**: 2025-12-27 23:00:31 (Asia/Riyadh / GMT+03:00)
- **MD5**: `070c2bd535618916f6d84b6a3728ee83`
- **SHA256**: `c24844cb0a20f05482ef1e6ca2bb60a02a567ac6941d30de817e528e6b449eb7`
- **Encoding**: UTF-8

**File code content:**

```python
from .network import NeuralNetwork
from .trainer import Trainer

from .layers.dense import Dense
from .layers.activations import Sigmoid, ReLU, Tanh, Softmax
from .layers.batch_norm import BatchNormalization
from .layers.dropout import Dropout

from .optimizers.sgd import SGD
from .optimizers.momentum import Momentum
from .optimizers.adam import Adam   
from .optimizers.adagrad import AdaGrad

from .losses.mse import MSE
from .losses.softmax_ce import SoftmaxCrossEntropy

class HyperparameterTuning:
    '''
    ÙƒÙ„Ø§Ø³ Ø¨ÙŠØ³ØªØ®Ø¯Ù… Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙØ¶Ù„ Ù‡Ø§ÙŠØ¨Ø± Ø¨Ø§Ø±Ø§Ù…ÙŠØªØ±Ø² (Ù…Ø«Ù„ learning rate Ùˆ batch size)
    Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¬Ø±Ø¨Ø© Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ù…Ù†Ù‡Ù… ÙˆØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¹Ù„Ù‰ (validation data)
    Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ø¨Ø§Ø±Ø§Ù…ØªØ±Ø§Øª Ø¨ÙƒÙˆÙ† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø£Ø¹Ù„Ù‰ Ø¯Ù‚Ø© ÙˆØ£Ø¯Ù†Ù‰ Ø®Ø³Ø§Ø±Ø©
    '''
    def __init__(self, x_train, y_train, x_val, y_val):
        self.x_train = x_train
        self.y_train = y_train
        self.x_val = x_val
        self.y_val = y_val

    def find_best_params(self, model_builder, opt_class, loss_class, lrs, batch_sizes, epochs=100):
        best_acc = -1.0
        best_loss = float('inf')
        best_config = {}

        for lr in lrs:
            for batch_size in batch_sizes:
                print(f"\nTesting:  LR={lr} ,  Batch={batch_size}")
                
                model = model_builder() 
                optimizer = opt_class(learning_rate=lr) 
                loss_fn = loss_class() 
                trainer = Trainer(model, optimizer, loss_fn) 
                
                trainer.fit(self.x_train, self.y_train, self.x_val, self.y_val, epochs=epochs, batch_size=batch_size)
                
                current_loss = trainer.calculate_loss(self.x_val, self.y_val)
                current_acc = trainer.calculate_accuracy(self.x_val, self.y_val)
                
                if (current_acc > best_acc) or (current_acc == best_acc and current_loss < best_loss):
                    best_acc = current_acc
                    best_loss = current_loss
                    best_config = {
                        'lr': lr, 
                        'batch_size': batch_size,
                        'acc': current_acc
                    }

        print(f"\n === Final Best Configuration === ")
        print(f"Config: LR={best_config['lr']}, Batch={best_config['batch_size']}, With accuracy = {best_acc:.2f}%")
        
        return best_config
```

---

## ğŸš« Binary/Excluded Files

The following files were not included in the text content:

- `test/__pycache__/__init__.cpython-314.pyc`
- `test/__pycache__/tester.cpython-314.pyc`

### <a id="ğŸ“„-test-init-py"></a>ğŸ“„ `test/__init__.py`

**File Info:**
- **Size**: 0 B
- **Extension**: `.py`
- **Language**: `python`
- **Location**: `test/__init__.py`
- **Relative Path**: `test`
- **Created**: 2025-12-27 08:10:51 (Asia/Riyadh / GMT+03:00)
- **Modified**: 2025-12-27 15:45:46 (Asia/Riyadh / GMT+03:00)
- **MD5**: `d41d8cd98f00b204e9800998ecf8427e`
- **SHA256**: `e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855`
- **Encoding**: ASCII

**File code content:**

```python

```

---

### <a id="ğŸ“„-test-tester-py"></a>ğŸ“„ `test/tester.py`

**File Info:**
- **Size**: 1.66 KB
- **Extension**: `.py`
- **Language**: `python`
- **Location**: `test/tester.py`
- **Relative Path**: `test`
- **Created**: 2025-12-27 07:54:22 (Asia/Riyadh / GMT+03:00)
- **Modified**: 2025-12-27 17:33:23 (Asia/Riyadh / GMT+03:00)
- **MD5**: `342b2c7557f18a06f43bbd4394a1c34e`
- **SHA256**: `3442eb91b65dd95e76269434c82f293794a20008c04f13e9b7a88c6682fdd642`
- **Encoding**: UTF-8

**File code content:**

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

from AldaabolMiniNN import (
    NeuralNetwork,Trainer, HyperparameterTuning,
    Dense, Sigmoid, ReLU, Tanh, BatchNormalization, Dropout,
    SGD, Adam, Momentum, AdaGrad,
    MSE, SoftmaxCrossEntropy, 
)

# Ù„Ù„ØªØ´ØºÙŠÙ„ Ù…Ù† Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¨Ø±ÙˆØ¬ÙƒØª AldaabolMiniNNproject
# py -m test.tester


# load and prepare data
iris = load_iris()
encoder = OneHotEncoder(sparse_output=False)
y_onehot = encoder.fit_transform(iris.target.reshape(-1, 1))
X_train, X_test, y_train, y_test = train_test_split(iris.data, y_onehot, test_size=0.2, random_state=42)

# built the test model
def build_model():
    model = NeuralNetwork()
    model.add(Dense(4, 32))     
    model.add(Sigmoid())
    model.add(BatchNormalization())
    model.add(Dense(32, 16))
    model.add(ReLU())
    model.add(Dense(16, 3))      
    return model

# search for best hyperparameters
tuning = HyperparameterTuning(X_train, y_train, X_test, y_test)

best_params = tuning.find_best_params(
    model_builder = build_model, 
    opt_class     = SGD,                
    loss_class    = SoftmaxCrossEntropy, 
    lrs           = [0.1, 0.01, 0.001], 
    batch_sizes   = [8, 16, 32], 
    epochs        = 300
)

# train final model with best hyperparameters
final_optimizer = SGD(learning_rate=best_params['lr'])
final_loss      = SoftmaxCrossEntropy()
final_model     = build_model()

trainer = Trainer(final_model, final_optimizer, final_loss)
trainer.fit(X_train, y_train, X_test, y_test, epochs=1000, batch_size=best_params['batch_size'])
```

---

### <a id="ğŸ“„-license-txt"></a>ğŸ“„ `LICENSE.txt`

**File Info:**
- **Size**: 1.07 KB
- **Extension**: `.txt`
- **Language**: `text`
- **Location**: `LICENSE.txt`
- **Relative Path**: `root`
- **Created**: 2025-12-27 07:53:00 (Asia/Riyadh / GMT+03:00)
- **Modified**: 2025-12-27 08:26:14 (Asia/Riyadh / GMT+03:00)
- **MD5**: `fd16f0dbeead73b863c1e2f9da55d2df`
- **SHA256**: `009b7d5e6a1d948e82fa208476f7e80e0391e3aee28954738fb48dc12b162e4f`
- **Encoding**: ASCII

**File code content:**

```text
MIT License

Copyright (c) 2025 Lubana Aldaabol

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

---

### <a id="ğŸ“„-readme-md"></a>ğŸ“„ `README.md`

**File Info:**
- **Size**: 6.49 KB
- **Extension**: `.md`
- **Language**: `text`
- **Location**: `README.md`
- **Relative Path**: `root`
- **Created**: 2025-12-27 07:52:40 (Asia/Riyadh / GMT+03:00)
- **Modified**: 2025-12-27 23:03:41 (Asia/Riyadh / GMT+03:00)
- **MD5**: `611af66ef85b357b2b6faa58f50aa0c8`
- **SHA256**: `1fc281a3dd962b87ea9322d64cbbe04df6a7d5421a0cd972deae738450b1f4e4`
- **Encoding**: ASCII

**File code content:**

````markdown
Project Overview: AldaabolMiniNN Library
AldaabolMiniNN** is a modular Deep Learning framework built entirely from scratch using **Python** and **NumPy**. The project mimics professional libraries like **PyTorch** by organizing components into a reusable package structure

How I Built It:
**Modular Architecture:** I structured the library into independent sub-packages: `layers`, `optimizers`, `losses`, and `network`. Each component is isolated to ensure scalability.
**The Core Engine:** Every layer inherits from a base `Layer` class, enforcing a strict contract for `forward` and `backward` passes using the **Chain Rule**.
**Numerical Stability:** I implemented a specialized `SoftmaxCrossEntropy` loss that handles potential numerical overflows (using the max-subtraction trick)
**Advanced Components:** I integrated a **Batch Normalization** layer to stabilize the internal covariate shift, allowing the model to train faster with higher learning rates
**Automation (The Tuner):** I developed a `HyperparameterTuning` module to automatically find the best `Learning Rate` and `Batch Size` through a grid-search approach.

Final Performance:
**Architecture:** `Dense > Sigmoid > BatchNorm > Dense > ReLU > Dense > SoftmaxWithLoss`.
**Dataset:** Iris Dataset (Categorical Classification).
**Results:** Successfully achieved **100% Validation Accuracy** with a final loss **0.0065**


Ø´Ø±Ø­ Ø¹Ø§Ù…:
**AldaabolMiniNN** Ù‡Ùˆ Ø¥Ø·Ø§Ø± Ø¹Ù…Ù„ Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ ØªÙ… Ø¨Ù†Ø§Ø¤Ù‡ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ Ù…Ù† Ø§Ù„ØµÙØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… **Python** Ùˆ **NumPy**. ÙŠØ­Ø§ÙƒÙŠ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ù‡ÙŠÙƒÙ„ÙŠØ© Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠØ© Ù…Ø«Ù„ *PyTorch* Ù…Ù† Ø®Ù„Ø§Ù„ ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª ÙÙŠ Ø­Ø²Ù…Ø© Ø¨Ø±Ù…Ø¬ÙŠØ© (Package) Ù‚Ø§Ø¨Ù„Ø© Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…

ÙƒÙŠÙ Ø£Ù†Ø¬Ø²Øª Ø§Ù„Ø¹Ù…Ù„:
**Ø§Ù„Ù‡ÙŠÙƒÙ„ÙŠØ© Ø§Ù„Ù…Ø¬Ø²Ø£Ø© (Modular Architecture):** Ù‚Ù…Øª Ø¨ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø¥Ù„Ù‰ Ø­Ø²Ù… ÙØ±Ø¹ÙŠØ© Ù…Ø³ØªÙ‚Ù„Ø©: `layers` (Ø§Ù„Ø·Ø¨Ù‚Ø§Øª)ØŒ `optimizers` (Ø§Ù„Ù…Ø­Ø³Ù†Ø§Øª)ØŒ `losses` (Ø¯ÙˆØ§Ù„ Ø§Ù„Ø®Ø³Ø§Ø±Ø©)ØŒ Ùˆ `network` (Ø§Ù„Ø´Ø¨ÙƒØ©)
**Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ:** ØªØ±Ø« ÙƒÙ„ Ø·Ø¨Ù‚Ø© Ù…Ù† ÙƒÙ„Ø§Ø³ Ø£Ø³Ø§Ø³ÙŠ `Layer` ÙŠÙØ±Ø¶ ØªÙ†ÙÙŠØ° Ø¹Ù…Ù„ÙŠØªÙŠ Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ ÙˆØ§Ù„Ø®Ù„ÙÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… **Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø³Ù„Ø³Ù„Ø© (Chain Rule)** Ø¨Ø¯Ù‚Ø© Ø±ÙŠØ§Ø¶ÙŠØ©
**Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ø­Ø³Ø§Ø¨ÙŠ:** Ù†ÙØ°Øª Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø© `SoftmaxCrossEntropy` Ù…ØªØ·ÙˆØ±Ø© ØªØ¹Ø§Ù„Ø¬ Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø¶Ø®Ù…Ø© (Overflow) Ù„Ø¶Ù…Ø§Ù† Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„ØªØ¯Ø±ÙŠØ¨
 **Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©:** Ø£Ø¶ÙØª Ø·Ø¨Ù‚Ø© **Batch Normalization** Ù„Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¯Ø§Ø®Ù„ÙŠØ§Ù‹ØŒ Ù…Ù…Ø§ Ø³Ù…Ø­ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø³Ø±Ø¹Ø© Ø£ÙƒØ¨Ø± ÙˆØ¨Ù…Ø¹Ø¯Ù„Ø§Øª ØªØ¹Ù„Ù… Ø£Ø¹Ù„Ù‰ Ø¯ÙˆÙ† ØªØ°Ø¨Ø°Ø¨
 **Ø§Ù„Ø£ØªÙ…ØªØ© (Tuning):** Ù‚Ù…Øª Ø¨ØªØ·ÙˆÙŠØ± ÙˆØ­Ø¯Ø© `HyperparameterTuning` Ù„Ù„Ø¨Ø­Ø« ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¹Ù† Ø£ÙØ¶Ù„ "Ù…Ø¹Ø¯Ù„ ØªØ¹Ù„Ù…" Ùˆ "Ø­Ø¬Ù… Ø¯ÙØ¹Ø©" (Batch Size) Ù„Ø¶Ù…Ø§Ù† Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ø­Ù„ Ø§Ù„Ø£Ù…Ø«Ù„

  Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ:
**Ù‡ÙŠÙƒÙ„ÙŠØ© Ø§Ù„Ø´Ø¨ÙƒØ©:** `Dense > Sigmoid > BatchNorm > Dense > ReLU > Dense > SoftmaxWithLoss`.
**Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:** Iris Dataset (ØªØµÙ†ÙŠÙ Ø§Ù„ÙØ¦Ø§Øª).
**Ø§Ù„Ù†ØªØ§Ø¦Ø¬:** Ù†Ø¬Ø­Øª Ø§Ù„Ù…ÙƒØªØ¨Ø© ÙÙŠ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø¯Ù‚Ø© **100%** Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹ Ø®Ø³Ø§Ø±Ø© Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¶Ø¦ÙŠÙ„Ø© Ø¨Ù„ØºØª **0.0065**.



Ù„Ù„ØªØ´ØºÙŠÙ„ Ù…Ù† Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¨Ø±ÙˆØ¬ÙƒØª AldaabolMiniNNproject
py -m test.tester


*OUTPUT EXAMPLE:*
Testing:  LR=0.1 ,  Batch=8
Epoch 0:   Loss 0.3059,   Validation Accuracy: 100.00%
Epoch 100:   Loss 0.0243,   Validation Accuracy: 93.33%
Epoch 200:   Loss 0.0746,   Validation Accuracy: 100.00%
Epoch 300:   Loss 0.0272,   Validation Accuracy: 100.00%

Testing:  LR=0.1 ,  Batch=16
Epoch 0:   Loss 0.3667,   Validation Accuracy: 96.67%
Epoch 100:   Loss 0.0107,   Validation Accuracy: 96.67%
Epoch 200:   Loss 0.2920,   Validation Accuracy: 93.33%
Epoch 300:   Loss 0.0065,   Validation Accuracy: 100.00%

Testing:  LR=0.1 ,  Batch=32
Epoch 0:   Loss 0.6357,   Validation Accuracy: 63.33%
Epoch 100:   Loss 0.0132,   Validation Accuracy: 93.33%
Epoch 200:   Loss 0.0168,   Validation Accuracy: 93.33%
Epoch 300:   Loss 0.1973,   Validation Accuracy: 93.33%

Testing:  LR=0.01 ,  Batch=8
Epoch 0:   Loss 1.1896,   Validation Accuracy: 83.33%
Epoch 100:   Loss 0.1609,   Validation Accuracy: 96.67%
Epoch 200:   Loss 0.0309,   Validation Accuracy: 96.67%
Epoch 300:   Loss 0.1746,   Validation Accuracy: 96.67%

Testing:  LR=0.01 ,  Batch=16
Epoch 0:   Loss 1.2922,   Validation Accuracy: 53.33%
Epoch 100:   Loss 0.0963,   Validation Accuracy: 96.67%
Epoch 200:   Loss 0.3638,   Validation Accuracy: 96.67%
Epoch 300:   Loss 0.0072,   Validation Accuracy: 96.67%

Testing:  LR=0.01 ,  Batch=32
Epoch 0:   Loss 2.5952,   Validation Accuracy: 30.00%
Epoch 100:   Loss 0.1365,   Validation Accuracy: 100.00%
Epoch 200:   Loss 0.0600,   Validation Accuracy: 96.67%
Epoch 300:   Loss 0.0435,   Validation Accuracy: 96.67%

Testing:  LR=0.001 ,  Batch=8
Epoch 0:   Loss 1.2392,   Validation Accuracy: 30.00%
Epoch 100:   Loss 0.5963,   Validation Accuracy: 86.67%
Epoch 200:   Loss 0.1522,   Validation Accuracy: 100.00%
Epoch 300:   Loss 1.6811,   Validation Accuracy: 96.67%

Testing:  LR=0.001 ,  Batch=16
Epoch 0:   Loss 2.2079,   Validation Accuracy: 3.33%
Epoch 100:   Loss 0.4813,   Validation Accuracy: 93.33%
Epoch 200:   Loss 0.3295,   Validation Accuracy: 93.33%
Epoch 300:   Loss 0.3474,   Validation Accuracy: 96.67%

Testing:  LR=0.001 ,  Batch=32
Epoch 0:   Loss 1.8867,   Validation Accuracy: 6.67%
Epoch 100:   Loss 0.4335,   Validation Accuracy: 86.67%
Epoch 200:   Loss 0.3340,   Validation Accuracy: 100.00%
Epoch 300:   Loss 0.2512,   Validation Accuracy: 100.00%

 === Final Best Configuration ===
Config: LR=0.1, Batch=8, With accuracy = 100.00%
Epoch 0:   Loss 0.4586,   Validation Accuracy: 73.33%
Epoch 100:   Loss 0.0783,   Validation Accuracy: 93.33%
Epoch 200:   Loss 0.0077,   Validation Accuracy: 93.33%
Epoch 300:   Loss 0.1505,   Validation Accuracy: 93.33%
Epoch 400:   Loss 0.5045,   Validation Accuracy: 100.00%
Epoch 500:   Loss 0.9727,   Validation Accuracy: 96.67%
Epoch 600:   Loss 0.4230,   Validation Accuracy: 100.00%
Epoch 700:   Loss 0.0063,   Validation Accuracy: 100.00%
Epoch 800:   Loss 0.0051,   Validation Accuracy: 93.33%
Epoch 900:   Loss 0.0106,   Validation Accuracy: 100.00%
Epoch 1000:   Loss 0.0516,   Validation Accuracy: 100.00%
PS C:\Users\lubanadaabol\python_projects\AldaabolMiniNNproject> 

````

---

### <a id="ğŸ“„-setup-py"></a>ğŸ“„ `setup.py`

**File Info:**
- **Size**: 755 B
- **Extension**: `.py`
- **Language**: `python`
- **Location**: `setup.py`
- **Relative Path**: `root`
- **Created**: 2025-12-27 07:53:12 (Asia/Riyadh / GMT+03:00)
- **Modified**: 2025-12-27 23:40:17 (Asia/Riyadh / GMT+03:00)
- **MD5**: `96f20aebd07ea15bd5c98c5bdaa4b33f`
- **SHA256**: `ef8babceac5e10a36a1e00763f3a5d86bf02e03e609da6151559885708eccd03`
- **Encoding**: ASCII

**File code content:**

```python
from setuptools import setup, find_packages

setup(
    name="AldaabolMiniNN",  
    version="1.0.0",
    author="Lubana Aldaabol",
    author_id="lubanadaabol",
    description="A mini neural network library built from scratch for educational purposes",
    long_description=open("README.md", encoding="utf-8").read(),
    long_description_content_type="text/markdown",
    url="https://github.com/lubanadaabol6-alt/AldaabolMiniNN",
    packages=find_packages(), 
    install_requires=[
        "numpy",
        "scikit-learn"
    ],
    classifiers=[
        "Programming Language :: Python :: 3",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
    ],
    python_requires='>=3.6',
)
```

---

